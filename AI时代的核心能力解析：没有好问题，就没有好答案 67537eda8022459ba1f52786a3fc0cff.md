# AI时代的核心能力解析：没有好问题，就没有好答案

欢迎加微信：cheng_weiguang。加入学习社群,共用交流学习呀~

> 前段时间有个社群的小伙伴问了一个问题“**到了一个新项目，需要设立新的流程，有哪些准备工作？**”，当时准备去回答的时候发现这个问题并不是很好回答，因为会发现缺少了很多潜在的背景信息，比如为什么需要新的流程，是谁需求，是什么目的，等等这些潜在的背景信息的缺失让这个问题可能就算有个回答，也一定能解决对应的人的真实场景下的具体问题。
> 

在这个AI持续进化并在某些任务上已经完全超过人类的大背景下，我们必须要去思考我们的核心能力到底是什么？上周刚结束的高考作文也提到了类似的话题，这可能是当下我们所有人都绕不开的一个话题

![微信图片_20240607234130.png](%25E5%25BE%25AE%25E4%25BF%25A1%25E5%259B%25BE%25E7%2589%2587_20240607234130.png)

在解答这个问题之前，我们先尝试用不同的方式来解答一下上面哪个问题

1. 直接把原始问题去给到大模型

![LobeChat_随便聊聊_2024-06-07.jpg](LobeChat_%25E9%259A%258F%25E4%25BE%25BF%25E8%2581%258A%25E8%2581%258A_2024-06-07.jpg)

我们可以看到其实回答的已经非常不错了，我们可能可以基于里面的一些内容来作为参考

但是真实的场景下，我们可能需要的不仅仅是这些，比如其实我已经有团队了或者我已经立项了，这些我们没有提前交待过的背景信息，其实就是大模型给的答案好像只能参考，但是并不能带来更大价值的原因

假如我们假设一下可能的情况是“**我是一个项目经理，我被安排到了一个新项目，这是一个软硬件结合的项目，这个新项目的团队成员之前都不认识，我也跟大家不熟悉，我们现在项目刚立项，我想设定一些项目管理的流程和制度，能帮助大家更好的完成前期的磨合，以及按照一定的节奏开展项目实施工作，保证项目能更加高效的推进，完成项目的既定目标**”（只是举个例子，具体的背景信息真实情况是什么不重要）

1. 可以在看下补充了这些信息之后的效果

![LobeChat_随便聊聊_2024-06-07 (1).jpg](LobeChat_%25E9%259A%258F%25E4%25BE%25BF%25E8%2581%258A%25E8%2581%258A_2024-06-07_(1).jpg)

可以看到回答的内容明显有了更强的针对性，至少针对我们可能重点关注的内容有了更好的回答。

**当然我们也不一定非要就只指望通过单次的提问就完成获得一个较好的答案，我们也需要学会利用大模型基于上下文的理解能力，来持续的追问，完成更加深度的信息获取**

# 当前的时代背景下，核心能力到底是什么

> 问题的关键在于找到关键问题
> 

上面这句话有点绕，但是细想是有些道理的

大模型的基础原理是基于输入持续预测下一个单词，所以就会有个很大的特点

<aside>
💡 所有的问题，都有答案

</aside>

不管最终回答质量怎么样，我们发现我们不管问什么，大模型都会给我们一个回答。

所以在当前的时代背景下，我们可能最需要的是**提问的能力**和**判断的能力**

1. 提问的能力

大模型的每一个词的输出都是基于之前的输入内容来持续预测的，所以输入决定输出，好的答案就需要一个好的问题

这也是提示词的价值，提示词的英文是prompt，这不是一个新发明的词，这个词在英语词典中的解释是这样的

> to assist (one acting or reciting) by suggesting or saying the next words of something forgotten or imperfectly learned
通过建议或说出遗忘或未完全学习的内容的下一个单词来协助（表演或背诵）
> 

可以看到就是有引导的意思，对于提示词的价值感兴趣的同学可以看下这篇文章

我们去看各种提示词的教程，你会发现其实本质也就是在怎么提出一个好的问题，来更好的引导大模型给出更好的回答。

而且大模型的这种技术的特性把表达清楚这件事情的重要性进一步放大了，我们之前跟具体的人交流的场景下，可能还有些公共的背景信息可以作为沟通的基础，比如大家的基于某一个具体的业务场景下进行沟通交流，或者有的人理解能力更强能更好的理解你表达的内容，让我们忽视了清晰的表达的重要性，但是现在我们跟AI大模型进行沟通对话，我们需要更加清晰的表达才能获得更好的反馈和答案

大模型基于上下文的持续预测和自注意力的相关机制，对于我们输入的每一个细节都会极其敏感，各种关键的背景信息不能漏掉，每一个表达和标点符号可能对最终的结果都会有影响

<aside>
🥇 **它的输出只跟你的输入有关系，你自己就是你在使用大模型的上限**

</aside>

2. 判断能力

由于所有的问题都会有答案，但是并不是所有的答案都是正确的，由于大模型预训练的数据局限性和基于概率的持续预测，所以会产生幻觉问题

我们需要对于我们提出的问题的回答具有基础的判断，同时对于不同的大模型产品具体的能力边界有基础的认知（至少当前阶段是如此，随着模型的进化可能会越来越智能，幻觉问题也会越来越少，但是我们永远需要自己的独立判断），要对于所有的输出内容都有自己的判断

其实不仅仅是大模型，我们不管是在看到或者听到任何信息和资讯的时候，都需要有自己的判断，独立的判断永远是我们作为人类最重要的能力之一

# 怎么提出好问题

那么到底怎么提出一个好的问题呢？我们也可以反向的通过一些提示词的框架来帮助我们思考这个问题，毕竟一个好的提示词大概率会是一个好的问题

下面是几种比较流行的提示词框架

![20240608-222050.jpg](20240608-222050.jpg)

![20240608-222140.jpg](20240608-222140.jpg)

![20240608-222158.jpg](20240608-222158.jpg)

![20240608-222226.jpg](20240608-222226.jpg)

对于这些框架我们进行对比分析之后，会发现其实本质上主要就是两部分

<aside>
❓ 一个好的问题 = waht（是什么）+why（为什么）

</aside>

1. what（是什么）

这部分主要就是这个问题到底所处的环境是什么，比如是谁提出的，背景信息是什么，上下文是什么，具体内容是什么，要求是什么，约束有什么，什么阶段，当前什么现状

简单来说就是要把我们自己现在到底所处的是什么环境说清楚，把AI跟我们放到同一个屋檐下，看到和听到我们所能看到和感受到的一切

1. why（为什么）

如果waht是我们当前在哪的话，那why就是我们到底要去哪，或者我们的目标在哪，比如这个问题到底核心人员是谁，关注什么，核心价值是什么，侧重点是什么，核心目标是什么

我们还是用文章开头的问题来尝试分析一下，问题好像是在问流程，但是我们需要往后退一步才能更好的回答这个问题，我们需要持续追问几个问题：

- 目标到底是什么？
    - 流程是目标吗？如果不是，那我们为了什么设定流程？流程到底是用来解决什么问题的？这个问题是不是被我们忽视了？
    - 是新团队需要新的流程来提高协作效率？是项目经理需要流程来提高管理效率？还是领导就是需要新的流程来获得更多的控制感
    - 持续的追问下，我们才能找到我们真实的问题是什么，我们都知道流程本质都是为团队服务的，不能为了流程而是设计流程

<aside>
🚀 **要为你的最终目标服务，而不是为了具体的形式来设计流程**

</aside>

总结一下，就是我们需要反复的追问这个问题是我们的本质问题嘛？我们就是为了解决这个问题吗？那个本质的问题到底是什么，需要我们反复的追问，当然我们也可以让AI来帮助我们思考本质或者关键问题到底是什么，但是前提是我们需要意识到这个事情的存在

1. AI给的回答是how（怎么做）

我们会发现我们的问题是是什么和为什么，AI给我的回答本质是怎么做，但是上面的提示词框架里面我们会发现其实也有部分how的内容，比如会定义具体的步骤是什么，或者所有的回答都需要自己先检查一遍，这些其实都是how的一部分

我们的当前为什么还需要how，是因为AI的智能程度还不太够，可能可以解决我们人类5分钟就能解决的问题，但是暂时没办法解决很多我们需要至少30分钟以上才能解决的问题，所以我们在很多时候需要把具体的操作步骤也讲清楚，AI才能给我们一个更好的回答

这也是当前AI大模型在很多场景下落地的困难点之一，由于技术还不够成熟，我们需要将所谓的know-how与大模型进行结合才能获得更好的结果（这部分有机会后面再展开介绍）

# 怎么与AI有效协作

最后我们来总结下，我们跟AI进行协作的正确姿势

1. 找到原始的问题
- 退一步想一下当前的问题到底是什么
- 从what+why的维度进行整理

1. 基于问题的发散和追问
- 基于关键问题的问题，对于回答的内容进行进一步的准问
- 对于回答内容的筛选和独立判断

1. 形成自己的认知和思考
- 最后基于AI的回答和自己的信息量形成更加新的思考和认知
- 整理成体系化的完整内容