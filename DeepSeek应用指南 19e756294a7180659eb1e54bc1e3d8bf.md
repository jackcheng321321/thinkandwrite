# DeepSeek应用指南

> 真理的根系总深埋于朴素的土壤，与其急于攀折表象的枝叶，不如以敬畏之心触摸概念的脉动，待它在现实的晴雨里抽芽生长——直到某天蓦然回首，那些曾被拆解的知识碎片，已悄然化作你呼吸间的星辰——DeepSeek R1
> 

![](https://my-image.askcheng.xyz/cheng-img/2025/02/26ee752b3d3cbf6602198a6d3ade9c67.png)

# 推理模型 VS 非推理模型

DeepSeek R1是一个推理模型，这个模型上的分类也是从模型训练到最终表现上与之前我们用过的大部分大模型做区分，我们可以把之前用过的模型叫做非推理模型，有些可能会叫指令模型，我们可以来对比一下两种类型的模型的差异

基础概念的解释：

- 强化学习（RL）：让模型基于外部反馈持续探索解决方案，有点类似人在社会中持续实践学习的过程。（上一个比较出圈的强化学习的例子是在围棋上超越人类的AlphoaGo）
- 监督微调（SFT）：给模型很多的例子，让他学会按照给的案例去输出，有点类似人在学校中学习知识的过程。
- 对齐：让模型的表现更加能被人接受。比如不要语言混着说，不要太冒犯，不要有安全问题，不要引导人有不良情绪等等。不同的模型的对齐侧重点可能不太一样，对齐的处理方式可能也不太一样

<aside>
💡

大模型的能力持续提升本质是两部分，一部分是智能程度，一部分是对齐人类。前面一部分是为了能完成更多更复杂的任务，后面一部分是为了能更好地被使用。

</aside>

| 对比维度 | 推理模型 | 非推理模型 |
| --- | --- | --- |
| 定义 | 推理模型是在模型的后训练阶段侧重**强化学习**的具有思考、反思等相关能力的模型 | 非推理模型主要是为了区分推理模型，在后训练阶段主要侧重**监督微调**的模型，如果没有在提示词使用思维链的技巧时，模型是直接输出最终结果的 |
| 代表模型 | Open O系列、DeepSeek R1 | GPT系列、豆包、kimi、Claude等等 |
| 训练方式 | 主要差异在后训练阶段，前面的预训练阶段其实是一样的，都是会基于Transformer的神经网络训练一个基础模型
推理模型在后训练阶段引入了强化学习的训练方式来加强模型的思考和推理能力，获得了很好的效果。当然我们用到的R1其实也有做监督微调来完成对齐
**主要的差异是侧重点不同，推理更加侧重强化学习** | 
非推理模型在后训练阶段主要的目标是对齐，主要侧重监督学习（SFT），当前也有强化学习的部分，不过非推理模型中的强化学习主要的目标是为了更好地对齐
非**推理模型的侧重点是SFT** |
| 具体表现 | 回答用户的问题之前后有一个思考的过程，这个过程里面会看到**模型的思考、反思以及搜索相关的能力体现**，这些能力不是大模型厂商在训练阶段教会的，是模型自己通过强化学习的过程中涌现的 | 基于预训练的海量数据进行基于概率的持续预测，在一些日常文字任务中提供帮助 |
| 速度和成本 | 由于每一次回答都有思考过程，所以一般会速度更慢，成本更高。（这个速度和成本是指的完成一整次回答的，不是具体回答时候的速度） | 一般会更快，也更加便宜。本来模型部署层面，非推理模型也更加成熟，所以当前阶段成本上是有比较大的优势的 |
| 使用场景 | 1. **处理模糊任务**
2. **解答一些发散的问题**
3. 大海捞针，提取关键信息
4.找差别，在大量的数据里面发现数据关系
5.**需要一定的逻辑推理的问题** | 1. 常规写作类的任务
2. 简单的事实性问题
3. 内容总结和基础概括
4. 任务步骤和流程比较清晰的
 |
|  |  |  |

**如何选择**
对于你的使用场景来说，最重要的是什么？　
• 速度与成本 → GPT 模型速度更快且成本更低
• 执行明确定义的任务 → GPT 模型能很好地处理明确定义的任务
• 准确性和可靠性 → o 系列模型是可靠的**决策者**
• 复杂问题解决 → o 模型在**模糊和复杂**中运作

# 认知偏差

一些认知上的误区和偏差会影响我们更好的理解和使用大模型来协助完成相关任务

## 只是侧重不同，没有所谓的更好

虽然现在R1为代表的推理模型热度很高，推出时间更晚，看上去大家的使用反馈也更好，但是其实推理模型和之前的非推理模型并不是更好的关系，以DeepSeek来举例，R1不是V3的新版本，只是两个模型基于同一个基础模型（V3 base）在不同的侧重方向上进行了后续的训练，甚至两者之间还互相使用了数据来促进最终的表现，更多的差异是应用场景上的差异

用户的对于R1的感受更好有几个重要的维度，R1是第一个大家可以非常方便使用的推理模型，用过O1的太少了，这种新的有思考深度的回答确实会带来更好的体验，就像如果你给一个人问了一个问题，他马上回答你和思考半天之后回答你，你是能明显感受到思考深度和回答质量的差异的

同时这种逻辑推理能力也被很好地泛化到了一些其他的泛娱乐的场景，比如算命、写作模仿、讲段子之类的，所以更加加速了这个模型被更多的人了解和接受。如果只是代码或者数学能力变强，是不会有这种效果的

所以用户更好的使用感受其实不是说模型就一定是更好的，推理模型也还是有一些问题，包括对于复杂指令的执行、简单问题的过度思考以及可能更加严重的幻觉问题等等，有些可能是技术特性本身决定的，有些可能是这个技术路线发展过程中待解决的问题

从技术发展的趋势来看，这两个模型大概率最后会合并到一起，毕竟用户不需要自己去区分自己正在用什么模型，OpenAI计划今年发布的GPT-5可能就是把O系列和GPT系列的模型进行整合，虽然看上去只是已有的两个模型的整合，但是我们也不能低估了可能带来的效果上的差异，从我们前面对于强化学习和监督微调的定义解释就是能发现，如果能把这两种方式做到有效地整合，相等于一个人读书读的好，在社会上的实践经验也足够，上限可能是非常高的

## 蒸馏和本地部署

蒸馏技术也是随着这一轮DeepSeek的爆火被大众所了解，包括由于DeepSeek产品无法有效的响应暴涨的用户需求，一些关于本地部署的教程信息也被大家广泛传播

但是关于蒸馏模型的理解其实是有一些误区的，我们通过下面这张图，蒸馏之后的模型名称来进一步的理解一下这个差异，首先是蒸馏的过程

![](https://my-image.askcheng.xyz/cheng-img/2025/02/e4419df272c1d95547ec09aa17793b77.png)

我们解释一下上面的图，蒸馏其实是一个**其他的基础小模型**（比如qwen或者llama），通过R1在后训练阶段生成的一些**SFT的数据**，完成一次**后训练的过程，**最终的模型名称叫DeepSeekR1-Distill-**Qwen-7B**

我们根据这个过程就可以得到几个信息：

- 蒸馏的基础模型是个第三方的模型，也就是就算用R1的训练数据做了SFT，**本质还是个小模型**，不然也不可能部署到一些本地环境中
- 蒸馏的过程本质就是一个类似SFT的过程，比较特殊的地方是用到了R1推理模型的数据，这些数据里面是包括了一下模型怎么深度思考的数据，可以理解成通过用R1的数据训练，来教会了小模型怎么深度思考
- 最后这个模型的名称的最后才是重点，就是Qwen-7B代表了这个基础模型的情况**，**但是我们很多时候容易被前面的DeepSeekR1所吸引

最后蒸馏技术这次为什么引起关注也是由于基于R1蒸馏之后的小模型的表现是明显比之前原始模型基于SFT好的，甚至比小模型做强化学习的效果还好。相当于如果你自己有个小模型，你可以基于R1的这些数据来获得一个极具性价比的模型能力的提升，所以确实是有非常大的应用场景和潜力的

但是需要注意的是，所谓的效果好，是相比于这个小模型之前的版本，或者这个小模型其他的后训练方式，但是并不是相较于R1这个模型，毕竟R1的基础模型是V3，是一个671B的模型，这个模型参数上的差异，不是靠蒸馏就能完全弥补的

**当前大部分人对于蒸馏的误区就是把基于R1的数据蒸馏的其他模型当成R1的其他低版本，这是有问题的，R1只是只有两个版本，R1和R1-Zero，我们用到的基本都是R1**

> *所以如果我们做个近似的比喻的话，你可以把蒸馏理解成一个二本的学生，找了个北大的高材生做老师，做了一段时间的集中学习，最后考试获得了一个还不错的成绩。但是这个学生还是这个学生，并没有发生什么改变*
> 

所以本地部署如果不是自己有一些特定的场景需求的话，其实是不太需要考虑，DeepSeek R1作为一个开源模型，当前的发展趋势就是只要有算力的机构和企业都会进行部署，具体使用上其实是不需要担心的，大概率我们可以在不止一个地方用到这个模型，也会更加方便的用到这个模型。

## 幻觉问题

由于推理模型其实本质也是基于概率的持续预测，所以幻觉问题还是会存在的，不是说能深度思考就不存在幻觉了，甚至最近的一些研究还表明幻觉问题可能更加严重了

可能是一些过度思考带来的问题，同时由于模型的回答里面会包含思考过程，所以平均的回答长度也是比常规的非推理模型更长的，也就有更大的概率导致幻觉问题了

这对于我们使用它也带来了新的挑战，因为模型会更加容易出现“**一本正经的胡说八道**”的情况，特别是有思考过程，如果你不是对应的专业方向，会进一步加深一本正经的程度，我们可能会更难从回答的措辞上进行判断，需要更进一步的事实性的核实工作

基于联网进行深度思考是一种有效的解决方案，但是也需要区分场景，如果你的场景不是从互联网上找一下回答的话，可能开启联网检索也不是一个好的选择。同时互联网的信息也不是绝对可靠，我们还是需要一些基础的判断和核实

# 怎么使用

## 在哪都能用

关于本地部署的地方其实也提到了，如果你需要使用的是R1，不是其他的小模型，本地部署就不是一个好的选择。

以及由于R1的完全开源的协议和比较好的用户使用感受，我们可以预见我们在越来越多的产品中可以用到它，所以不需要有太大的局限，一定要在官网使用等等。

可能也有一些人会觉得官方和第三方会有效果上的差异，其实这个担心也不太有必要。因为DeepSeek官方也都发布了他们API的使用指南，包括不要使用系统提示词，以及联网检索的提示词和文件上传解读的提示词，我们甚至可以在一些第三方产品里面使用到一些官方可能没有上线的能力，比如上传文件或者图片进行解读和处理等等

如果还是有一些使用体感的差异，可能就真的只是概率预测带来的一些小差异或者真的用到的产品不是真的部署的R1，而是一些蒸馏后的模型。不过如果不是R1一般还是比较容易发现的，所以一些比较大的厂商基本不太会这么处理

所以找个不是那么热门的AI产品可能体验更好，因为用的人少，可以响应会更快，以及我们可能马上在微信上都可以用到了，具体在哪用不应该是一个问题。

下面推荐几个还不错的能用到DeepSeek的产品（可能有时效性，比如使用人数变多，或者产品策略调整之后，可能会出现一些变化）

1. [跃问](https://yuewen.cn/)

![](https://my-image.askcheng.xyz/cheng-img/2025/02/21d6cf068e4df365cc7ed1bfa60667ac.png)

直接把联网搜索和附件的问题的能力也带上了，整体体验挺好的，他们家自己的多模态模型的整体效果也不错，也可以尝试一下

1. ima.copilot或者腾讯元宝

![](https://my-image.askcheng.xyz/cheng-img/2025/02/f91e124ab47b17404609e8919989593a.png)

腾讯系的产品接入DeepSeek相较于其他产品最大的差异在于检索的时候是可以检索到公众号的数据的，这是其他的产品可能都做不到的，公众号的相关资讯的内容质量还是蛮高的，所以在一些特殊场景下可以考虑使用

## 搜索怎么用

我们看到很多接入了DeepSeek R1的产品都有联网检索的功能，这个能力其实在推理模型之前我们就有在使用，使用场景其实是类似的，只是R1模型+联网检索的数据可能可以带来更好的回答效果，因为模型对于检索到的数据的使用和整理上会更加智能和合理，甚至会判断不同的数据源之间的冲突和差异，大家可以在使用联网检索的时候看看模型的思考过程，就可以观察到这种效果

**联网搜索并不是开启的效果就一定是好的**，一些需要互联网数据的场景效果才会更好，比如市场调研、综合分析、行业解读等等

**一些比较模糊或者没有标准答案的问题，可以尝试不使用联网检索的效果**。比如一些管理问题、思考上的问题等等，这些问题没有绝对的好坏，可以尝试打开或者关闭联网都看看具体的回答来进行参考

同时联网检索也需要关注回答的准确性，因为涉及到的环节更多了，出错的概率其实变大了，比如检索到的原始数据其实只是相关的，被模型拿来回答这个问题了，比如检索到的内容有时效性的问题，但是大模型并没有识别出来，比如模型本身产生的一些幻觉问题等等。所以我们需要对回答的内容有进一步的分析和判断

## 提示词

推理模型对于提示词的需求确实是更少了，或者说进一步降低了用户的使用门槛，因为模型自己会思考，所以我们可以不需要在提示词里面给更多的引导模型思考以及怎么做的内容了，推理模型自己会去发散

但是提供更加完整的背景和目标信息还是非常有必要的，你的场景和目标更清晰，模型回答的效果就更好，包括告诉模型你是谁、你面对的是什么场景、你期望解决什么具体的问题。

如果你观察仔细的话，你甚至会发现如果你没有说你是谁，R1会在思考的过中基于你的提问猜测你是谁，所以**提供更加清晰和精准的要求是非常重要的**。

最后如果你对于具体任务的执行过程和步骤是有要求的，其实也可以考虑不要使用推理模型，可能会有更好的效果

## 其他

关于连续对话的体验也有用户感觉好像DeepSeek的效果不如之前的模型表现，这个其实是一些产品逻辑上的差异，有些AI产品可能在R1类型的模型上下文的处理上，没有带上每次问答的原始思考过程，只是把最终的回答带上了，可能就会带来一些效果上的差异。这种处理方式主要是出于成本上的考虑

同时如果是不同话题的内容，也最好是新建话题来进行对话，防止不同的话题造成的模型思考上的错乱

最后从实际使用的角度，我们需要基于我们的场景对于具体的模型和功能使用上进行选择。**当前从技术和产品的发展趋势来看，这些需要用户进行判断和选择的部分可能也是技术和产品需要持续去解决的问题**

# 一些启发

模型的训练过程其实跟人类学习的过程有很多类似和相通的地方，所以了解和学习模型的训练和最终表现对于我们每个人自己的持续成长也有启发，是一个帮助我们认识自己的过程

## 关于强化学习

我们知道R1的表现这个好，有很大一部分因素是对于强化学习技术的使用。我们其实在社会中的学习就跟强化学习这个过程非常像，我们基于各种外部的反馈来调整我们的行为

R1这次强化学习用一个对于之前大模型训练中的强化学习的改进，之前大家都聚焦到基于过程的激励，这次R1直接基于结果放弃了基于过程的激励。

基于过程的激励其实有点像有些人学习就一定要发朋友圈，就是做这个事情的时候过于强调过程，在意的不是最终的结果，是学习这个动作

基于结果就是具体的方法论不是那么重要了，也没有所谓的标准答案，核心就是你需要去解决具体的问题，达成目标（当然现实生活还是有道德法律相关的大的约束的）

> **不管模型中间做错了什么，只要不是重复的，那么最后模型做对了，我们就认为这是一个好的探索，值得鼓励。反之，如果模型一顿探索，最后做错了，那么再努力也是错，要惩罚。**
> 

## 关于读书和实践

R1的突出表现其实是有两部分组成的，一部分是基于海量的数据预训练之后的V3-base，一部分是之后的强化学习，这两部分最终的乘法决定了最终的效果

预训练的过程其实就有点类似我们持续进行阅读和学习的过程，强化学习就有点类似我们在社会中基于外部反馈持续成长的过程

所以这两部分是缺一不可的，甚至两部分是可以互相结合来获得更好的效果的。基础靠博览群书，强化学习靠持续的实践

## 关于蒸馏

最后关于蒸馏，我们前面解释了一下具体的原理，蒸馏带给我们的启发是如果你基础可能不太好，但是能找到一个好的师傅，是可以带来更大的能力上的提升的

但是你的基础还是会是你的瓶颈，不能因为有个好的师傅就放弃了对于基础能力的提升，一个好的师傅可以提高你的下限，但是上限其实是你自己决定的

# 模糊问题的价值

对于管理相关的问题和场景推理模型是明显会有更好的效果的。因为**管理问题往往是模糊的，没有标准答案的**，R1的深度思考可以带来更多的视角和参考，同时R1在回答过程时的思考过程也能反向促进我们更好地引导我们对于我们做的事情本身目标的思考

还有一类问题其实也是类似的特点，一些人生思考也可以去尝试对话和提问，来获得一些新的理解和思考

为了获得更好的效果，在提问上就需要有更加具体的场景和问题，清晰和精准的表达自己的问题

最后对于回答的内容其实跟之前我们使用大模型是一样的，不要指望大模型回答的每个内容都是有价值的，也不要直接一次性否定了所有的回答，**去看那些对于你有价值的部分就好了**。

# 总结

最后面对持续变化的新技术，更好的应对方式不是去焦虑替代自己的问题，去了解基础逻辑，更好地应用，其实对于是否会替代你就有一个清晰的答案了

R1只是这个模型在这个方向上的第一个版本，我们可以预见技术还好高速迭代和进化，我们需要做的是怎么更好地与AI进行协作

同时未来是不是一下子变成某个新的样子的，认知的突破和社会结构的转变都是有一个过程的，在这个持续变化的过程中，怎么能持续地适应新的变化是最重要的

**而且如果你真是有在持续的使用R1，你会发现思考和回答问题可能都不是我们实际工作和生活中的最重要的那部分，毕竟知道了那么多的道理，我们还是过不好一生，去做永远是最关键的**