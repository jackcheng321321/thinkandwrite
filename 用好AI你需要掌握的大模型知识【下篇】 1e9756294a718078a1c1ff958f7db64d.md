# 用好AI你需要掌握的大模型知识【下篇】

> 这是我们 AI时代下项目管理系列文章的第二篇，也是关于技术解析的第一部分的内容，是关于用好AI你需要掌握的大模型知识的下篇，如果你还没有看过上篇，可以到下面的系列文章的链接中点击阅读
> 

如果你对于我们这个系列内容感兴趣，欢迎关注和转发~

在【上篇】中我们介绍了关于大模型的两个核心知识和认知

- AI是基于海量数据“学习”模式，而非真正“思考”

AI的训练和学习方式其实是基于概率的持续预测，所以决定了我们在实际使用中看到的各种现象和问题，理解这个基础逻辑可以帮助我们更好地使用AI，规避一些日常使用中的错误理解，更好地发挥大模型的价值

- AI是“博学”，不是“全知全能”

AI的预训练由于有数据时效性的问题，所以AI也不是全知全能的，大模型有它的局限性，对于那些数据范围之外的任务和知识，我们应该借助联网检索或者提供更多上下文和背景信息的方式来提升模型的表现

接下来，我们今天要介绍另外两个核心认知

![](https://my-image.askcheng.xyz/cheng-img/2025/05/b5824b33c80a89bd9c40eec1691eeb8b.png)

# AI大模型看到的和你看到的不一样

这里我们需要引入一个叫做token的概念，你可以把token理解成大模型自己的语言，就像我们不同的国家的人群有不同的语言一样，你如果没有学习过的话，你看其他的语言的文字就跟看天书一样

AI大模型也有自己的特殊的理解的“语言”，这个语言就是token，不管是训练还是推理（回答用户的问题）的时候，第一步都是需要把数据和用户的数据转换成token，不管你输入的是语言或者数字，第一步都是这个。下面是OpenAI提供的一个token转换的工具，我们可以看到我们不同的输入之后，转换成token的样子

![](https://my-image.askcheng.xyz/cheng-img/2025/05/91cd0f0d04c3941f941b326b1753577c.png)

![](https://my-image.askcheng.xyz/cheng-img/2025/05/206f0cb4739c64a3608e0cb20d73d269.png)

token有几个地方值得我们注意：

- 我们可以看到token的分词和我们人类不一样，右边这这张图里面可以看到并不是一个字就是一个token，有可能是两个字才出一个token
- token的本质其实就是数字，我们可以从左边的图里面看到上面这16个字符被转换成了10个token，一个token就是一个数字

为什么我们需要理解token呢？因为我们了解token之后我们才会知道和理解  **大模型看到的和我们看到的是不一样的，所以才会有些我们认看上去很简单的任务为什么大模型做不好**

比如，strawberry里面有几个r，这句话里面一共有几个。之类的问题，核心就是我们看到的和大模型看到的不是一码事，我们可以看看这些单词和具体在转换成token之后变成了什么

![](https://my-image.askcheng.xyz/cheng-img/2025/05/f6ebc1a7ac09d717252b46dd855eb819.png)

![](https://my-image.askcheng.xyz/cheng-img/2025/05/429cf4f0afc61b6fdaa2c812d56aee87.png)

可以看到strawberry变成了3个token，而且最后一个token里面是有两个r的，包括右边的例子里面前的。和后面的。的token的拆分也是完全不一样的

![](https://my-image.askcheng.xyz/cheng-img/2025/05/a50a345693a0c5b689fc43f0fa3f70d7.png)

所以为什么有些人类看似简单的任务大模型做不好的核心逻辑在这个，大模型跟人类的理解还是不太一样的，虽然在很多实际的表现上很相似，但是本质上还是有差异的，就像我们上篇讲到的一样，AI

**看上去好像有思考，但是本质还是基于概率的持续预测**

认知到这一点之后，我们需要理解大模型也有它的局限性，也有它不擅长的地方。所以我们再有类似的任务，比如数字、数数、计算相关的时候，要让大模型去使用工具，而不是让模型自己来回答这个问题，往往大模型直接回答的结果是不好的

# AI大模型的“记忆”有局限性

![](https://my-image.askcheng.xyz/cheng-img/2025/05/ff153dba3376ffee6874a3de64c058bc.png)

AI大模型的“记忆”和我们人类所理解的记忆有点不太一样，大模型的记忆是靠在与模型每次交互的时候都带入记忆来实现的

你可以这么理解，大模型本身其实没有任何的记忆的机制和逻辑，但是在与你对话的时候，在每次与你进行一个对话的时候，就先把你们之前的对话记录看一遍，然后在来回答你的问题，这样就给你的感觉就是它有“记忆”了

就像有个机器人，每一轮与你对话的时候，都先把你们之前的对话记录看过一遍了，那其实这个机器人是不是一个其实也没有关系了，因为每一次都会基于你们之前的对话记录来跟你接着交流

而这个“记忆”的机制其实就是基于模型的上下文的能力实现的，大模型都有上下文的长度的限制，所以大模型的“记忆”也是有局限性的

我们看到很多大模型发布的时候，都是一个叫做长度的参数，指的就是模型单次能出处理的上下文的内容长度

![](https://my-image.askcheng.xyz/cheng-img/2025/05/6d4f8c3fdc0778e5b91cda1592005c67.png)

由于“记忆”的效果是AI产品基于大模型的产品定制开发和实现的，大模型原生其实是没有“记忆效果”的，所以我们在不同的产品里面体验到的记忆效果其实是不一样的

比如可能底层模型都是DeepSeek，但是有的产品在上下文里面导入了你和AI前面5轮的对话记录，而有的AI产品可能是带入的10轮，你可能在体验的时候就会有不同的体验和感受

所以就算是同一个模型，不同的产品我们也是有可能获得不同的体验的，这个体验上的差异来自不同的AI产品对于模型的使用和工程处理方式的差异

基于这个上下文的逻辑，我们还可以延伸出来一些我们在使用大模型的时候需要注意的点：

- 输入决定输出，我们与大模型的交互最终的回答完全取决于我们的输入，特别的地方在于不是所有的输入都是我们自己的输入，我们的输入可能外面是包了一层AI产品补充的信息的，比如你与AI之前的对话记录来实现记忆效果等等。
- “喂”对信息很重要，虽然可能不是所有的输入都是我们自己输入的，但是基于这个记忆实现的效果，我们其实也可以在输入中补充更多的信息来提升模型最终输出的效果。因为大模型有基于上下文进行学习和理解的能力，所以我们补充更多的背景信息，可以让模型更好地完成任务，这也是提示词的价值

“好的AI工具应该一次就给我完美答案，不需要我调整或指导。”所以这是一个认知误区，我们需要建立与AI协作的习惯，通过持续的迭代来获得我们最终需要的答案

![](https://my-image.askcheng.xyz/cheng-img/2025/05/1883629ac54d98c770354a6217dc4e18.png)

# 解决方案

上面的两个知识和认知里面我们有讲到AI可能有局限性的地方，那么这些局限性我们也有对应的解决方案

1. token的问题

token带来的问题是有些可能对于人类比较简单的数数，或者数字相关的任务好像模型做的不好，这个时候的解决方案其实跟上篇提到的一样，就是给模型用工具，之前我们上篇介绍的是用联网检索的范式，而这种场景下我们可以使用的工具是代码能力

就像人类在更复杂的计算场景中需要借助计算器一样，在涉及到数字相关的场景时，我们可以要求大模型用写代码的方式来完成对应的任务

现在很多产品都有内置的代码解释器，我们可以要求模型用写代码然后运行代码的方式来解决对应的问题

![](https://my-image.askcheng.xyz/cheng-img/2025/05/cdb6cb1ceeaa7a5faf071e9633a10560.png)

1. 记忆局限性的问题

这个问题其实有两个在具体使用维度的启发

- 我们在单次的问答无法获得比较好的效果的时候，其实可以尝试去追问的方式来持续的迭代最终的答案，因为大部分AI产品都是会基于上下文的能力来实现记忆效果的。当然这个场景也有一些例外，比如你发现好像怎么补充和调整都无法达到理想的效果，这个时候其实可以尝试总结和整理一下自己的要求，重新开一个窗口来对话，因为上下文不仅带来记忆效果，也有可能带来噪音和干扰
- 基于上面讲到的这一点，如果我们与AI聊的不是一个话题的时候，也需要记得去重开一个窗口来进行对话，因为你前的对话记录是有可能干扰你当前期望去对话的内容的，所以我们需要结合实际的情况来选择是继续对话还是新开窗口

![](https://my-image.askcheng.xyz/cheng-img/2025/04/57dee907b5e9ceb74a8fd4b87a570809.png)

---

这就是我们关于用好AI你需要掌握的大模型知识【下篇】的全部内容啦，如果你觉得内容对你有帮助，欢迎多多点赞、转发~同时也非常欢迎大家在留言区跟我们交流